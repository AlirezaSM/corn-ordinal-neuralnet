{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11ded39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a8cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 8 training examples\n",
    "X_train = torch.rand(8, 99)\n",
    "y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n",
    "\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9fc73",
   "metadata": {},
   "source": [
    "# Niu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0680b60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_to_levels(label, num_classes):\n",
    "    levels = [1]*label + [0]*(num_classes - 1 - label)\n",
    "    levels = torch.tensor(levels, dtype=torch.float32)\n",
    "    return levels\n",
    "\n",
    "levels = []\n",
    "\n",
    "for label in y_train:\n",
    "    levels_from_label = label_to_levels(label, num_classes=NUM_CLASSES)\n",
    "    levels.append(levels_from_label)\n",
    "\n",
    "levels = torch.stack(levels)\n",
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e9e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for simplicity, this is the ordinal network:\n",
    "\n",
    "# def __init__()\n",
    "niu = torch.nn.Linear(99, NUM_CLASSES-1)\n",
    "\n",
    "# def forward(self, X_train):\n",
    "logits = niu(X_train)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0716922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6742, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_niu(logits, levels):\n",
    "    val =  -torch.sum((torch.log(torch.sigmoid(logits))*levels + \n",
    "                       torch.log(1 - torch.sigmoid(logits))*(1-levels)),\n",
    "                      dim=1)\n",
    "    return torch.mean(val)\n",
    "\n",
    "\n",
    "\n",
    "loss_niu(logits, levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35832791",
   "metadata": {},
   "source": [
    "# Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f5ffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([True, True, True, True, True, True, True, True]),\n",
       "  tensor([0, 1, 1, 1, 1, 1, 1, 1])),\n",
       " (tensor([False,  True,  True,  True,  True,  True,  True,  True]),\n",
       "  tensor([0, 1, 1, 1, 1, 1, 1])),\n",
       " (tensor([False, False,  True,  True,  True,  True,  True,  True]),\n",
       "  tensor([0, 0, 0, 1, 1, 1])),\n",
       " (tensor([False, False, False, False, False,  True,  True,  True]),\n",
       "  tensor([0, 1, 1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = []\n",
    "for i in range(NUM_CLASSES-1):\n",
    "    label_mask = y_train > i-1\n",
    "    label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "    sets.append((label_mask, label_tensor))\n",
    "sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1819d454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as Niu et al.\n",
    "\n",
    "# def __init__(self):\n",
    "conditional_net = torch.nn.Linear(99, NUM_CLASSES-1)\n",
    "\n",
    "# def forward(self, X_train):\n",
    "logits = conditional_net(X_train)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36dc8687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0 loss: 0.9681528806686401\n",
      "task 1 loss: 0.8592419624328613\n",
      "task 2 loss: 0.7534388899803162\n",
      "task 3 loss: 0.5798597931861877\n"
     ]
    }
   ],
   "source": [
    "def loss_conditional1(logits, sets):\n",
    "    \n",
    "    losses = []\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "        pred = logits[train_examples, task_index]\n",
    "        \n",
    "        if len(s[1]) < 1:\n",
    "            continue        \n",
    "        \n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pred))*train_labels + \n",
    "                           torch.log(1 - torch.sigmoid(pred))*(1-train_labels))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "\n",
    "losses = loss_conditional1(logits, sets)\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "    print(f'task {i} loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013658a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0 loss: 0.9681528806686401\n",
      "task 1 loss: 0.8592420220375061\n",
      "task 2 loss: 0.7534387707710266\n",
      "task 3 loss: 0.579859733581543\n"
     ]
    }
   ],
   "source": [
    "# More stable  implementation combining log and sigmoid\n",
    "\n",
    "def loss_conditional2(logits, sets):\n",
    "    \n",
    "    losses = []\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "        pred = logits[train_examples, task_index]\n",
    "        \n",
    "        if len(s[1]) < 1:\n",
    "            continue\n",
    "        \n",
    "        loss = -torch.mean( F.logsigmoid(pred)*train_labels \n",
    "                           + (F.logsigmoid(pred) - pred)*(1-train_labels) )\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses\n",
    "\n",
    "\n",
    "losses = loss_conditional2(logits, sets)\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "    print(f'task {i} loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914f42f-2f9a-49f2-a9c4-a12f9532375d",
   "metadata": {},
   "source": [
    "average over training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06a14a0-ffc9-4c05-852d-69a6050252c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8342, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_conditional_v2(logits, sets):\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "        \n",
    "        loss = -torch.sum( F.logsigmoid(pred)*train_labels\n",
    "                                + (F.logsigmoid(pred) - pred)*(1-train_labels) )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n",
    "\n",
    "\n",
    "losses = loss_conditional_v2(logits, sets)\n",
    "\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a688ea2",
   "metadata": {},
   "source": [
    "# Conditional with branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceceb6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before:\n",
    "\n",
    "conditional_net = torch.nn.Linear(99, NUM_CLASSES-1)\n",
    "logits = conditional_net(X_train)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ae013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def __init__():\n",
    "branches = [torch.nn.Linear(99, 10) for i in range(NUM_CLASSES-1)]\n",
    "output_layers = [torch.nn.Linear(10, 1) for i in range(NUM_CLASSES-1)]\n",
    "\n",
    "# def forward(self, X_train):\n",
    "logits = torch.empty(X_train.shape[0], len(output_layers))\n",
    "\n",
    "for i in range(len(output_layers)):\n",
    "    logits[:, i] = output_layers[i](torch.relu(branches[i](X_train))).squeeze()\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5183b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0 loss: 0.7900136113166809\n",
      "task 1 loss: 0.7687548398971558\n",
      "task 2 loss: 0.6789558529853821\n",
      "task 3 loss: 0.6958199143409729\n"
     ]
    }
   ],
   "source": [
    "losses = loss_conditional1(logits, sets)\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "    print(f'task {i} loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e623e374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0 loss: 0.7900136709213257\n",
      "task 1 loss: 0.7687548398971558\n",
      "task 2 loss: 0.6789558529853821\n",
      "task 3 loss: 0.6958198547363281\n"
     ]
    }
   ],
   "source": [
    "losses = loss_conditional2(logits, sets)\n",
    "\n",
    "for i, loss in enumerate(losses):\n",
    "    print(f'task {i} loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52ffd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def __init__():\n",
    "branches = [torch.nn.Linear(99, 10) for i in range(NUM_CLASSES-1)]\n",
    "output_layers = [torch.nn.Linear(10, 1) for i in range(NUM_CLASSES-1)]\n",
    "\n",
    "# def forward(self, X_train):\n",
    "logits = []\n",
    "\n",
    "for i in range(len(output_layers)):\n",
    "    logits.append(output_layers[i](torch.relu(branches[i](X_train))).squeeze())\n",
    "\n",
    "torch.stack(logits).T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab3761-78a2-42ce-a82f-007d6b102493",
   "metadata": {},
   "source": [
    "# Usage in Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1bc1d2-4b98-41ba-a964-44c8e53d989b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1821, -0.3466,  0.0601,  0.3062],\n",
       "        [ 0.4080, -0.1538,  0.2523,  0.6325],\n",
       "        [ 0.3554, -0.1273,  0.2426,  0.6384],\n",
       "        [ 0.2563, -0.3050,  0.3775,  0.5575],\n",
       "        [ 0.4663, -0.3350, -0.0339,  0.2436],\n",
       "        [ 0.5024, -0.1526, -0.1593,  0.2748],\n",
       "        [ 0.3983, -0.2236, -0.0391,  0.1512],\n",
       "        [ 0.2917, -0.4038, -0.0181,  0.2092]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as Niu et al.\n",
    "\n",
    "# def __init__(self):\n",
    "conditional_net = torch.nn.Linear(99, NUM_CLASSES-1)\n",
    "\n",
    "# def forward(self, X_train):\n",
    "logits = conditional_net(X_train)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "454b506d-519b-4034-853c-87313d282a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac73546-9949-4a82-ada3-998dc45db832",
   "metadata": {},
   "source": [
    "### Conditional v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563d992-33ca-4197-a235-7d099b0134ad",
   "metadata": {},
   "source": [
    "with `cumprod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04626220-6226-4121-8dcc-c593fd4a82f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5454, 0.2259, 0.1163, 0.0670],\n",
       "        [0.6006, 0.2773, 0.1560, 0.1019],\n",
       "        [0.5879, 0.2753, 0.1542, 0.1009],\n",
       "        [0.5637, 0.2392, 0.1419, 0.0902],\n",
       "        [0.6145, 0.2563, 0.1260, 0.0706],\n",
       "        [0.6230, 0.2878, 0.1325, 0.0753],\n",
       "        [0.5983, 0.2658, 0.1303, 0.0701],\n",
       "        [0.5724, 0.2292, 0.1136, 0.0627]], grad_fn=<CumprodBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.sigmoid(logits)\n",
    "probas = torch.cumprod(probas, dim=1)\n",
    "\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e6a0672-48cc-4382-977f-5b3f073057df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proba_to_label(probas):\n",
    "    \"\"\"\n",
    "    Converts predicted probabilities from extended binary format\n",
    "    to integer class labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probas : torch.tensor, shape(n_examples, n_labels)\n",
    "        Torch tensor consisting of probabilities returned by CORAL model.\n",
    "\n",
    "    Examples\n",
    "    ----------\n",
    "    >>> # 3 training examples, 6 classes\n",
    "    >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295],\n",
    "    ...                        [0.496, 0.485, 0.267, 0.124, 0.058],\n",
    "    ...                        [0.985, 0.967, 0.920, 0.819, 0.506]])\n",
    "    >>> proba_to_label(probas)\n",
    "    tensor([2, 0, 5])\n",
    "    \"\"\"\n",
    "    predict_levels = probas > 0.5\n",
    "    predicted_labels = torch.sum(predict_levels, dim=1)\n",
    "    return predicted_labels\n",
    "\n",
    "predicted_labels = proba_to_label(probas).float()\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e0118-bd26-4a0c-b377-5c9b95a69bee",
   "metadata": {},
   "source": [
    "### Conditional v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171306-c85b-41c4-a06d-84e35a8de873",
   "metadata": {},
   "source": [
    "like v1 but without `cumprod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4babb8b-7769-45d4-9e16-576f5c24a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5454, 0.4142, 0.5150, 0.5760],\n",
       "        [0.6006, 0.4616, 0.5627, 0.6531],\n",
       "        [0.5879, 0.4682, 0.5603, 0.6544],\n",
       "        [0.5637, 0.4243, 0.5933, 0.6359],\n",
       "        [0.6145, 0.4170, 0.4915, 0.5606],\n",
       "        [0.6230, 0.4619, 0.4603, 0.5683],\n",
       "        [0.5983, 0.4443, 0.4902, 0.5377],\n",
       "        [0.5724, 0.4004, 0.4955, 0.5521]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.sigmoid(logits)\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9be79b0-28d6-496f-b22d-7f2d2552a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = proba_to_label(probas).float()\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b5f0f-13af-48ea-9997-997b12b68493",
   "metadata": {},
   "source": [
    "### Conditional v2-argmax (Debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e73d5-2fb7-4404-af96-57e2eb8a5fa0",
   "metadata": {},
   "source": [
    "calculate the probs of (y=0, y=1, y=2,…) using (1-P(y>0), P(y>0)(1-P(y>1|y>0), P(y>0)P(y>1|y>0)(1-P(y>2|y>1),…), and then using argmax to find the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "628b052a-7f23-4050-9bb9-db6db3dbd7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5454, 0.4142, 0.5150, 0.5760],\n",
       "        [0.6006, 0.4616, 0.5627, 0.6531],\n",
       "        [0.5879, 0.4682, 0.5603, 0.6544],\n",
       "        [0.5637, 0.4243, 0.5933, 0.6359],\n",
       "        [0.6145, 0.4170, 0.4915, 0.5606],\n",
       "        [0.6230, 0.4619, 0.4603, 0.5683],\n",
       "        [0.5983, 0.4443, 0.4902, 0.5377],\n",
       "        [0.5724, 0.4004, 0.4955, 0.5521]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.sigmoid(logits)\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61146ab3-5f61-43b8-89f7-98493e408b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4546, 0.3195, 0.1096, 0.0493, 0.0670],\n",
       "        [0.3994, 0.3234, 0.1212, 0.0541, 0.1019],\n",
       "        [0.4121, 0.3127, 0.1210, 0.0533, 0.1009],\n",
       "        [0.4363, 0.3245, 0.0973, 0.0517, 0.0902],\n",
       "        [0.3855, 0.3583, 0.1303, 0.0553, 0.0706],\n",
       "        [0.3770, 0.3352, 0.1553, 0.0572, 0.0753],\n",
       "        [0.4017, 0.3324, 0.1355, 0.0602, 0.0701],\n",
       "        [0.4276, 0.3432, 0.1156, 0.0509, 0.0627]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones((probas.shape[0], 1))#.to(device)\n",
    "comp_1 = torch.cat((ones, torch.cumprod(probas, dim=1)), dim=1)\n",
    "comp_2 = torch.cat((1-probas, ones), dim=1)\n",
    "probas_y = torch.mul(comp_1, comp_2)\n",
    "probas_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba8f749-3e67-46de-bab5-986ff113ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = torch.argmax(probas_y, dim=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf630d3-7d72-442a-8400-08747ec66dfc",
   "metadata": {},
   "source": [
    "### Conditional v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363adc1-f232-4439-aa96-0bd7cbfedbfd",
   "metadata": {},
   "source": [
    "Same as above, but in code, average over training examples rather than tasks when computing the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33260bfe-a6cf-4e0a-85a0-6b158e1af589",
   "metadata": {},
   "source": [
    "### Conditional v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabeb20-1a04-4176-a1ed-c62682ee9962",
   "metadata": {},
   "source": [
    "same as `Conditional v2-argmax` but in code, average over training examples rather than tasks when computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e76c32-bfc1-4887-9d0f-825a4cd6d135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
